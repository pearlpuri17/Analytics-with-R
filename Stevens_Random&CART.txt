stevens= read.csv("stevens.csv")
str(stevens)
library(caTools)
*******library needed to split data********
set.seed(3000)

spl=sample.split(stevens$Reverse, SplitRatio = 0.7)
train = subset(stevens,spl==TRUE)
test = subset(stevens, spl == FALSE)

install.packages("rpart")
library(rpart)
install.packages("rpart.plot")
library(rpart.plot)

stree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method= "class", minbucket= 25)

**********method= class tells R to build a classification tree and not a regression tree******************
******minbucket limits the tree so that it doesn't overfit to our training set. We selected a value of 25, but we could pick a smaller or 
larger value******
prp(stree)
predictcart= predict(stree, newdata= test, type= "class") 
table(test$Reverse, predictcart)

accuracy = 
(41+71)/(41+71+22+36)
*****Lastly, to evaluate our model, let's generate an ROC curve for our CART model using the ROCR package.****
library(ROCR)
predictroc = predict(stree, newdata = test)
predictroc

For each observation in the test set, it gives two numbers which can be thought of as the probability of outcome 0 and the probability of outcome 1.
More concretely, each test set observation is classified into a subset, or bucket, of our CART tree.

******So just like for logistic regression, we'll start by using the prediction function. We'll call the output pred, and then use 
prediction, where the first argument is the second column of PredictROC, which we can access with square brackets,
and the second argument is the true outcome values, Test$Reverse.******

pred = prediction(predictroc[,2], test$Reverse)

****Now we need to use the performance function, where he first argument is the outcome of the prediction function, and then the next 
two arguments re true positive rate and false positive rate, what we want on the x and y-axes of our ROC curve.****

perf= performance(pred,"tpr","fpr")
plot(perf)

***************************************************************************
Q) What is the AUC?
A) 
as.numeric(performance(pred,"auc")@y.values)

Q) How many splits does the tree have if we change the value of minbucket parameter to 5.

A) 
stree2 = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method= "class", minbucket= 5)
prp(stree2)

Q) How many splits does the tree have when minbucket parameter changed to 100?

A)
stree3 = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method= "class", minbucket= 100)
prp(stree3)

******************************************************Random Forest******************************************************************
In R

Package Required- randomForest

install.packages("randomForest")
library(randomForest)

****Creating a random forest****

sforest= randomForest(Reverse~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize= 25, ntree= 200) 

The randomForest function does not have a method argument.So when we want to do a classification problem, we need to make sure outcome 
is a factor. Let's convert the variable Reverse to a factor variable

We do this by typing the name of the variable we want to convert-- in our case Train$Reverse-- and then type as.factor and then in 
parentheses the variable name, Train$Reverse.

train$Reverse = as.factor(train$Reverse)
test$Reverse = as.factor(test$Reverse)

sforest= randomForest(Reverse~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize= 25, ntree= 200)

**************Lets predict now*************

PredictForest = predict(sforest, newdata = test)
table(test$Reverse, PredictForest)

Accuracy=
(40+74)/(40+74+19+37)

our CART model had an accuracy of 65.9%. So our random forest model improved our accuracy a little bit over CART.

**************************************************************************************************************
Q) First, set the seed to 100, and the re-build the random forest model, exactly like we did in the previous video (Video 5). 
Then make predictions on the test set. What is the accuracy of the model on the test set?

A)
set.seed(100)

sforest2= randomForest(Reverse~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize= 25, ntree= 200)

PredictForest2= predict(sforest2, newdata = test)

Accuracy=
table(test$Reverse, PredictForest2)
(42+75)/(42+75+35+18)

Q) Now, set the seed to 200, and then re-build the random forest model, exactly like we did in the previous video (Video 5). 
Then make predictions on the test set. What is the accuracy of this model on the test set?

A) 
set.seed(200)

sforest3= randomForest(Reverse~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize= 25, ntree= 200)

PredictForest3= predict(sforest3, newdata = test)

Accuracy=
table(test$Reverse, PredictForest3)
(44+76)/(44+76+17+33)


Cross validation
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
numFolds = trainControl(method="cv",number=10)
cpGrid= expand.grid(.cp=seq(0.01,0.5,0.01))
train(Reverse ~.-Docket-Term,data= train, method="rpart",trControl=numFolds,tuneGrid=cpGrid)

streeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method= "class", cp=0.19)
predictCV= predict(streeCV, newdata= test, type= "class") 
table(test$Reverse, predictCV)
accuracy=
(59+64)/(59+64+29+18)
 
hence increased accuracy
prp(streeCV)