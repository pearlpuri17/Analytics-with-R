wine <- read.csv("wine.csv")
str(wine)
summary(wine)
model1 <- lm(Price ~ AGST, data=wine)
summary(model1)
model1$residuals
SSE<- sum(model1$residuals^2)
SSE
model2 <- lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
SSE = sum(model2$residuals^2)
SSE
model3<- lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
SSE <- sum(model3$residuals^2)
SSE

model4<- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
cor(wine$HarvestRain,wine$WinterRain)

wineTest<- read.csv("wine_test.csv")
str(wineTest)
predictTest <- predict(model4, newdata=wineTest)
predictTest

SSE = sum((wineTest$Price- predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)

1-SSE/SST
****************************************************************
Moneyball

baseball = read.csv("baseball.csv")
str(baseball)
summary(baseball)

moneyball = subset(baseball,Year<2002)
str(moneyball)
moneyball$RD = moneyball$RS-moneyball$RA
cor(moneyball$RD,moneyball$W)
plot(moneyball$RD,moneyball$W)

WinsReg<- lm(W ~ RD, data= moneyball)
summary(WinsReg)

RunsReg<- lm(RS~OBP+SLG+BA,data=moneyball)
summary(RunsReg)

RunsReg<- lm(RS~OBP+SLG,data=moneyball)
d2001<-subset(moneyball,Year==2001)
mean(d2001$OBP)
0.3320667
mean(d2001$SLG)
0.4264333

**************************************************
> summary(RunsReg)

Call:
lm(formula = RS ~ OBP + SLG, data = moneyball)

Residuals:
    Min      1Q  Median      3Q     Max 
-70.838 -17.174  -1.108  16.770  90.036 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -804.63      18.92  -42.53   <2e-16 ***
OBP          2737.77      90.68   30.19   <2e-16 ***
SLG          1584.91      42.16   37.60   <2e-16 ***
******************************************************
Runs scored in 2002 = -804.63 +2737.77*.33+1584.91*.43
-804.63+903.46+681.51=780
******************************************************
Runs against in 2002= -837.38+2913.6*.31+1514.29*.373
= 622

Wins= 80.88+.1058*158
=98 games
******************************************************
teamRank = c(1,2,3,3,4,4,4,4,5,5)	
wins2012<-c(94,88,95,88,93,94,98,97,93,94)
wins2013<-c(97,97,92,93,92,96,94,96,92,90)

cor(teamRank,wins2012)
cor(teamRank,wins2013)

*******************************************************
NBA<- read.csv("NBA_train.csv")
str(NBA)
table(NBA$W,NBA$Playoffs)
NBA$PtsDiff<-NBA$PTS-NBA$oppPTS
plot(NBA$W,NBA$PtsDiff)

WinsReg<- lm(W ~ PtsDiff,data=NBA)
summary(WinsReg)

41+.0326(ptsdiff)>42
42 is the amount of points diff we ascertained we need to make it to the playoffs

PointsReg<- lm(PTS ~ X2PA+X3PA+FTA+AST+ORB+DRB+TOV+STL+BLK,data=NBA)
summary(PointsReg)

PointsReg$residuals
SSE=sum(PointsReg$residuals^2)
SSE
RMSE=sqrt(SSE/nrow(NBA))
RMSE
RMSE of 184 doesnt seem too big. lets see what is our average points
mean(NBA$PTS)

Lets remove some insignificant variables
Lets remove turnovers. lets remove turnover first as it has the maximum p value of all

PointsReg2<- lm(PTS ~ X2PA+X3PA+FTA+AST+ORB+DRB+STL+BLK,data=NBA)
summary(PointsReg2)

PointsReg3<- lm(PTS ~ X2PA+X3PA+FTA+AST+ORB+STL+BLK,data=NBA)
summary(PointsReg3)

PointsReg4<- lm(PTS ~ X2PA+X3PA+FTA+AST+ORB+STL,data=NBA)
summary(PointsReg4)

SSE4<-sum(PointsReg4$residuals^2)
SSE4
RMSE4<-sqrt(SSE4/nrow(NBA))
RMSE4

This model is better- we have less variables,all significant and equal amount of error.

NBA_test<-read.csv("NBA_test.csv")
PointsPrediction<- predict(PointsReg4, newdata = NBA_test)
summary(PointsPrediction)
PointsPrediction
The R-squared value we had before from our model,the 0.8991, you might remember, is the measure
of an in-sample R-squared, which is how well the model fits the training data.
But to get a measure of the predictions goodness of fit,we need to calculate the out of sample R-squared.

SSE = sum((PointsPrediction - NBA_test$PTS)^2)
SSE
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
SST
R2 = 1-SSE/SST
R2
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
**********************************************************************************************************
Exercise 3

When handling a skewed dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself -- this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models.

Note that the "exp" function stands for the exponential function. The exponential can be computed in R using the function exp().

Using which to find location of a value in data frame

which(FluTest$Week == "2012-03-11 - 2012-03-17")

relative error is calculated as (Observed var- Estimated var)/Observed var

FluTrain <- read.csv("FluTrain.csv")
summary(FluTrain)
str(FluTrain)
FluTrain$Week[which.max(FluTrain$ILI)]
FluTrain$Week[which.max(FluTrain$Queries)]
hist(FluTrain$ILI)
plot(log(FluTrain$ILI),FluTrain$Queries)
FluTrend1<- lm(log(ILI)~ Queries,data=FluTrain)
summary(FluTrend1)
cor(FluTrain$Queries,log(FluTrain$ILI))
.842*.842
FluTest<-read.csv("FluTest.csv")
PredTest1 = exp(predict(FluTrend1,newdata = FluTest))
PredTest1
which(FluTest$Week == "2012-03-11 - 2012-03-17")
PredTest1[11]
(FluTest$ILI[11]-PredTest1[11])/FluTest$ILI[11]

SSE = sum((PredTest1 - FluTest$ILI)^2)
SSE 
RMSE=sqrt(SSE/nrow(FluTest))
RMSE
********************************Important*******************************
The observations in this dataset are consecutive weekly measurements of 
the dependent and independent variables. This sort of dataset is called a 
"time series." 

Often, statistical models can be improved by predicting the current value
of the dependent variable using the value of the dependent variable from
earlier weeks. 

In our models, this means we will predict the ILI variable in the current 
week using values of the ILI variable from previous weeks.

First, we need to decide the amount of time to lag the observations. 
Because the ILI variable is reported with a 1- or 2-week lag, a decision 
maker cannot rely on the previous week's ILI value to predict the current 
week's value. Instead, the decision maker will only have data available 
from 2 or more weeks ago. 

We will build a variable called ILILag2 that contains the ILI value from 
2 weeks before the current observation.

To do so, we will use the "zoo" package, which provides a number of helpful methods for time series models. While many functions are built into R, you need to add new packages to use some functions. New packages can be installed and loaded easily in R, and we will do this many times in this class. Run the following two commands to install and load the zoo package. In the first command, you will be prompted to select a CRAN mirror to use for your download. Select a mirror near you geographically.
*********************************/Important*******************************
install.packages("zoo")
library(zoo)
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
names(FluTrain)
str(FluTrain)
summary(FluTrain)
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))
FluTrend2 <- lm(log(ILI)~ Queries + log(ILILag2), data= FluTrain) 
summary(FluTrend2)
ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad = TRUE)
FluTest$ILILag2=coredata(ILILag2)
str(FluTest)
summary(FluTest)
FluTest$ILILag2[1]
=FluTrain$ILI[416]
FluTest$ILILag2[2]
=FluTrain$ILI[417]

PredTest2= exp(predict(FluTrend2,newdata=FluTest))
PredTest2
RMSE2= sqrt(mean((PredTest2-FluTest$ILI)^2))
RMSE2

In this problem, we used a simple time series model with a single lag term. ARIMA models are a more general form of the model we built, which can include multiple lag terms as well 
as more complicated combinations of previous values of the dependent variable. If you're interested in learning more, check out ?arima or the available online tutorials for these sorts of models.





































